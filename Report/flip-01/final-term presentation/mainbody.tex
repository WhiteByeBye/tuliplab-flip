%=================================================================

\section{Introduction}\label{sec-intro}

\subsection{Problem Statement}
\
This is a problem with natural language processing.
The ubiquitousness of smartphones enables 
people to announce an emergency they’re 
observing in real-time. So the target is
Predict whether a real disaster has occurred 
based on keywords, location, and twitter text.

\subsection{Data List}
\

There are 3 data sets with a total of 5 attributes,
the fllowings ~\cref{tbl:Attribute Information} are the  
name and meaning of attributes.


\begin{description}
	\item[train.csv] the training set.
	\item[test.csv] the test set.
	\item[sample\_submission.csv] a sample submission file in the correct format.
\end{description}

\begin{table}[htbp]
	\label{tbl:Attribute Information}
	\centering
	\caption{Attribute Information}
	\begin{tabular}{llllll}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		Attributes & Information                                                                            \\
		\hline
		id   & a unique identifier for each tweet                                                               \\
		text    & the text of the tweet                                                                         \\
		location     & the location the tweet was sent from (may be blank)                                      \\
		keyword     & a particular keyword from the tweet (may be blank)                                        \\
		target    &  in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)    \\                                               \\
		\hline
		%\bottomrule
	\end{tabular}
\end{table}

\subsection{Problem Analysis}

\subsubsection{Problem Possible Solutions}
\

There are many machine learning models and algorithms 
that can solve NLP problems, such as Word2vec, 
FastText, BERT, etc. Word embedding and text cleaning 
through pre-trained models may greatly improve 
prediction accuracy. Then use BERT to 
get the final prediction result. 


\subsubsection{Evaluation Methods}


Before experiment, determine the evaluation methods
to assess the model performance is very important,
Submissions are evaluated using F1 
between the predicted and expected answers.

\begin{itemize}
	\item
	F1 is calculated as follows: \\[5 pt]
	$F _ { 1 } = 2 * \frac { \text { precision } * \text { recall } } { \text { precision } + \text {recall} }$
	\\[5 pt]
	\item
	where: \\[5 pt]
	$\begin{aligned} \text {precision} & = \frac { T P } { T P + F P } \\ \text {recall} & = \frac { T P } { T P + F N } \end{aligned}$
\end{itemize}


\section{Exploratory Data Analysis} \label{sec-data_exploration}

\subsection{Keyword and Location}
\

\subsubsection{Missing Values}
Both training and test set have same ratio 
of missing values in keyword and location.
the fllowings ~\cref{fig:missing-values-of-keyword-and-location} are the  
missing values of keyword and location.

\begin{itemize}
	\item
    0.8\% of keyword is missing in both training and test set
	\item
	33\% of location is missing in both training and test set
\end{itemize}


\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.3]{"Figures/Missing Values of Keyword and Location.pdf"}
	\caption{Missing Values of Keyword and Location}
	\label{fig:missing-values-of-keyword-and-location}
\end{figure}

Since missing value ratios between training and test set 
are too close, they are most probably taken from the 
same sample. Missing values in those features are 
filled with no\_keyword and no\_location respectively.

\subsubsection{Cardinality and Target Distribution}

Locations are not automatically generated, they are user inputs. 
That's why `location` is very dirty and there are too many 
unique values in it. It shouldn't be used as a feature.

\begin{itemize}
	\item
	Number of unique values in keyword: 222 (Training) - 222 (Test)
	\item
	Number of unique values in location: 3342 (Training) - 1603 (Test)
\end{itemize}

The fllowings ~\cref{fig:Target-Distribution-in-Keywords} shows that 
there is signal in `keyword` because some of those words can only 
be used in one context. Keywords have very different tweet counts 
and target means. `keyword` can be used as a feature by itself or 
as a word added to the text. Every single keyword in training set 
exists in test set. If training and test set are from the same 
sample, it is also possible to use target encoding on `keyword`.

\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.3]{"Figures/Target Distribution in Keywords.pdf"}
	\caption{Target Distribution in Keywords}
	\label{fig:Target-Distribution-in-Keywords}
\end{figure}

\subsection{Meta Features}
\

Distributions of meta features in classes and datasets 
can be helpful to identify disaster tweets. It looks 
like disaster tweets are written in a more formal way 
with longer words compared to non-disaster tweets 
because most of them are coming from news agencies. 
Non-disaster tweets have more typos than disaster tweets 
because they are coming from individual users. The meta 
features used for the analysis are;

\begin{description}
	\item[word\_count] number of words in text.
	\item[unique\_word\_count] number of unique words in text.
	\item[stop\_word\_count] number of stop words in text.
	\item[url\_count] number of urls in text.
	\item[mean\_word\_length] average character count in words.
	\item[char\_count] number of characters in text.
	\item[punctuation\_count] number of punctuations in text.
	\item[hashtag\_count] number of hashtags "\#" in text.
	\item[mention\_count] number of mentions "@" in text.
\end{description}

The fllowings ~\cref{fig:Meta-feature-target-distribution} shows that 
all of the meta features have very similar distributions in 
training and test set which also proves that training and 
test set are taken from the same sample.

All of the meta features have information about target as 
well, but some of them are not good enough such as `url\_count`, 
`hashtag\_count` and `mention\_count`.

On the other hand, `word\_count`, `unique\_word\_count`, 
`stop\_word\_count`, `mean\_word\_length`, `char\_count`, 
`punctuation\_count` have very different distributions 
for disaster and non-disaster tweets. Those features 
might be useful in models.

\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.1]{"Figures/Meta feature target distribution.pdf"}
	\caption{Meta feature target distribution}
	\label{fig:Meta-feature-target-distribution}
\end{figure}



\subsection{Target and N-grams}
\subsubsection{Target}
\

The fllowings ~\cref{fig:Target-distribution} shows that 
class distributions are 57\% for 0 (Not Disaster) and 43\% for 1 (Disaster). 
Classes are almost equally separated so they don't require any 
stratification by `target` in cross-validation. 


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Target distribution.pdf}
	\caption{Target distribution}
	\label{fig:Target-distribution}
\end{figure}

\subsubsection{N-grams}
\paragraph{Unigrams}
The fllowings ~\cref{fig:Unigrams} shows that:
\begin{itemize}
	\item
	Most common unigrams exist in both classes are mostly punctuations, 
	stop words or numbers. It is better to clean them before modelling 
	since they don't give much information about `target`.
	\item
	Most common unigrams in disaster tweets are already giving 
	information about disasters. It is very hard to use some 
	of those words in other contexts.
	\item
	Most common unigrams in non-disaster tweets are verbs. 
	This makes sense because most of those sentences have informal 
	active structure since they are coming from individual users.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Unigrams.pdf}
	\caption{Unigrams}
	\label{fig:Unigrams}
\end{figure}

\paragraph{Bigrams}
There are no common bigrams exist in both classes  because the context is clearer.
The fllowings ~\cref{fig:Bigrams} shows that:
\begin{itemize}
	\item
	Most common bigrams in disaster tweets are giving more information about 
	the disasters than unigrams, but punctuations have to be stripped from words.
	\item
	Most common bigrams in non-disaster tweets are mostly about reddit or 
	youtube, and they contain lots of punctuations. Those punctuations have to be cleaned out of words as well.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Bigrams.pdf}
	\caption{Bigrams}
	\label{fig:Bigrams}
\end{figure}

\paragraph{Trigrams}
There are no common trigrams exist in both classes  because the context is clearer.
The fllowings ~\cref{fig:Bigrams} shows that:
\begin{itemize}
	\item
	Most common trigrams in **disaster** tweets are very similar to bigrams. 
	They give lots of information about disasters, but they may not provide any 
	additional information along with bigrams.
	\item
	Most common trigrams in **non-disaster** tweets are also very similar to bigrams, and they contain even more punctuations.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Trigrams.pdf}
	\caption{Trigrams}
	\label{fig:Trigrams}
\end{figure}


\section{Embeddings and Text Cleaning}
\subsubsection{Embeddings Coverage}

When you have pre-trained embeddings, doing standard preprocessing 
steps  might not be a good idea because some of the valuable 
information can be lost. It is better to get vocabulary as close 
to embeddings as possible. In order to do that, train vocab and 
test vocab are created by counting the words in tweets.

Text cleaning is based on the embeddings below:

\begin{itemize}
	\item GloVe-300d-840B
	\item FastText-Crawl-300d-2M 
\end{itemize}

The fllowings ~\cref{tab:Embeddings-cover} shows that:
                
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
	\centering
	\caption{Embeddings cover}
	\begin{tabular}{lcccc}
		\toprule
		& \multicolumn{2}{c}{Training Set} & \multicolumn{2}{c}{Test Set} \\
		\midrule
		& vocabulary & text  & vocabulary & text \\
		\midrule
		Glove & 52.06\% & 82.68\% & 57.21\% & 81.85\% \\
		FastText & 51.52\% & 81.84\% & 56.55\% & 81.12\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:Embeddings-cover}%
\end{table}%
	
	
\subsubsection{Text Cleaning}
\
 
Tweets require lots of cleaning but it is inefficient to 
clean every single tweet because that would consume too 
much time. A general approach must be implemented for cleaning.

* The most common type of words that require cleaning in `oov` 
have punctuations at the start or end. Those words doesn't have 
embeddings because of the trailing punctuations. Punctuations `\#`, 
`@`, `!`, `?`, `+`, `\&`, `-`, `\$`, `=`, `<`, `>`, `|`, `{`, `}`, 
`^`, `'`, `(`, `)`,`[`, `]`, `*`, `\%`, `...`, `'`, `.`, `:`, `;` 
are separated from words

\begin{itemize}
	\item
	Special characters that are attached to words are removed completely.
	\item
	Contractions are expanded.
	\item
	Urls are removed.
	\item
	Character entity references are replaced with their actual symbols.
	\item
	Typos and slang are corrected, and informal abbreviations are written in their long forms.
	\item
	Some words are replaced with their acronyms and some words are grouped into one.
	\item
	Finally, hashtags and usernames contain lots of information about the context but they 
	are written without spaces in between words so they don't have embeddings.Informational 
	usernames and hashtags should be expanded but there are too many of them. 
	I expanded as many as I could, but it takes too much time to run `clean` 
	function after adding those replace calls.
\end{itemize}

As shown in the ~\cref{tab:Text-Cleaning} below:
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
	\centering
	\caption{Text Cleaning}
	\begin{tabular}{cccc}
		\toprule
		\multicolumn{2}{c}{Special characters} & \multicolumn{2}{c}{Contractions} \\
		\midrule
		\textbackslash{}x89ÛÏWhen &  When & he's  &  he is \\
		China\textbackslash{}x89Ûªs &  China's & there's &  there is \\
		let\textbackslash{}x89Ûªs &  let's & We're &  We are \\
		fromåÊwounds &  from wounds & That's &  That is \\
		JapÌ\_n &  Japan & won't &  will not \\
		Ì©    &  e    & they're &  they are \\
		SuruÌ¤ &  Suruc & Can't &  Cannot \\
		å£3million &  3 million & wasn't &  was not \\
		\cmidrule{1-2}    \multicolumn{2}{c}{Character entity references} & don\textbackslash{}x89Ûªt &  do not \\
		\cmidrule{1-2}    \&gt  & , >   & aren't &  are not \\
		\&lt  & , <   & isn't &  is not \\
		\&amp & , \&  & What's &  What is \\
		\cmidrule{1-2}    \multicolumn{2}{c}{Typos, slang and informal abbreviations} & haven't &  have not \\
		\cmidrule{1-2}    w/e   &  whatever & hasn't &  has not \\
		w/    &  with & There's &  There is \\
		USAgov &  USA government & He's  &  He is \\
		recentlu &  recently & It's  &  It is \\
		Ph0tos &  Photos & You're &  You are \\
		amirite &  am I right & I'M   &  I am \\
		exp0sed &  exposed & shouldn't &  should not \\
		8/5/2015 &  2015-08-05 & I'm   &  I am \\
		WindStorm &  Wind Storm & Isn't &  is not \\
		8/6/2015 &  2015-08-06 & Here's &  Here is \\
		10:38PM &  10:38 PM & you've &  you have \\
		10:30pm &  10:30 PM & you\textbackslash{}x89Ûªve &  you have \\
		16yr  &  16 year & we're &  we are \\
		lmao  &  laughing my ass off & what's &  what is \\
		TRAUMATISED &  traumatized & couldn't &  could not \\
		\cmidrule{1-2}    \multicolumn{2}{c}{Acronyms} & we've &  we have \\
		\cmidrule{1-2}    MH370 &  Malaysia Airlines Flight 370 & he'll &  he will \\
		mÌ¼sica &  music & Y'all &  You all \\
		okwx  &  Oklahoma City Weather & Weren't &  Were not \\
		arwx  &  Arkansas Weather & Didn't &  Did not \\
		gawx  &  Georgia Weather & they'll &  they will \\
		scwx  &  South Carolina Weather & they'd &  they would \\
		cawx  &  California Weather & DON'T &  DO NOT \\
		tnwx  &  Tennessee Weather & That\textbackslash{}x89Ûªs &  That is \\
		……    & ……    & ……    & …… \\
		\bottomrule
	\end{tabular}%
	\label{tab:Text-Cleaning}%
\end{table}%


After text cleaning, the embedding coverage has been greatly improved.
As shown in the ~\cref{Embeddings-cover-2} below:
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
	\centering
	\caption{Embeddings cover 2}
	\begin{tabular}{lcccc}
		\toprule
		& \multicolumn{2}{c}{Training Set} & \multicolumn{2}{c}{Test Set} \\
		\midrule
		& vocabulary & text  & vocabulary & text \\
		\midrule
		Glove & 52.06\% & 82.68\% & 57.21\% & 81.85\% \\
		FastText & 51.52\% & 81.84\% & 56.55\% & 81.12\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:Embeddings-cover-2}%
\end{table}%


\subsubsection{Cross-validation}
\

First of all, when the training/test sets are concatenated, 
and tweet counts by `keyword` are computed, it can be seen 
that training and test set are split inside `keyword` groups. 
We can also come to that conclusion by looking at `id` feature. 
This means every `keyword` are stratified while creating 
training and test set. We can replicate the same split for cross-validation.

Tweets from every `keyword` group exist in both training and 
test set and they are from the same sample. In order to replicate 
the same split technique, `StratifiedKFold` is used and `keyword` 
is passed as `y`, so stratification is done based on the `keyword` 
feature. `shuffle` is set to `True` for extra training diversity. 
Both folds have tweets from every `keyword` group in training and 
validation sets which can be seen from below.
	

\section{Model}
\subsection{BERT Layer}
This model uses the implementation of BERT from the TensorFlow Models 
repository on GitHub at `tensorflow/models/official/nlp/bert`. It uses 
L=12 hidden layers (Transformer blocks), a hidden size of H=768, and A=12 attention heads.

This model has been pre-trained for English on 
the Wikipedia and BooksCorpus. Inputs have been "uncased", 
meaning that the text has been lower-cased before tokenization 
into word pieces, and any accent markers have been stripped. In order to 
download this model, `Internet` must be activated on the kernel.

\subsection{Architecture}

\begin{description}
		\item[DisasterDetector] a wrapper that incorporates the cross-validation and metrics stated above
		\item[max\_seq\_length] parameter can be used for tuning the sequence length of text
		\item[plot\_learning\_curve] plots Accuracy, Precision, Recall and F1 Score (for validation set) 
		stored after every epoch alongside with training/validation loss curve. This helps to see which 
		metric fluctuates most while training
\end{description}


\subsection{Evaluation}
\
The  ~\Cref{tbl:Evaluation} below shows the changes 
in the model's performance throughout the forecast process.

\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.3]{"Figures/Evaluation.pdf"}
	\caption{Evaluation}
	\label{fig:Evaluation}
\end{figure}

\section{Conclusion}

\begin{itemize}
	\item Don't use standard preprocessing steps like stemming or 
	stopword removal when you have pre-trained embeddings.
	\item Get your vocabulary as close to the embeddings as possible.
	\item Text cleaning requires patience and meticulousness. 
	\item BERT does have an advantage over other models when 
	used properly on NLP problems.
\end{itemize}

%\lstset{language=python}         
%\begin{lstlisting}[frame=single]  % Start your code-block
%rf = RandomForestClassifier(random_state = 0)
%clf = GridSearchCV(rf, param_grid = params, scoring = accuracy_scorer, cv = 10, n_jobs = -1)
%clf.fit(X_train, y_train)
%y_pred = clf.predict(X_test)
%\end{lstlisting}










