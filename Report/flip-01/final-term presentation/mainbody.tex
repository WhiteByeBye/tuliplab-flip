%=================================================================

\section{Introduction}\label{sec-intro}

\subsection{Problem Statement}
\
This is a problem with natural language processing.
The ubiquitousness of smartphones enables 
people to announce an emergency theyâ€™re 
observing in real-time. So the target is
Predict whether a real disaster has occurred 
based on keywords, location, and twitter text.

\subsection{Data List}
\

There are 3 data sets with a total of 5 attributes,
the fllowings ~\cref{tbl:Attribute Information} are the  
name and meaning of attributes.


\begin{description}
	\item[train.csv] the training set.
	\item[test.csv] the test set.
	\item[sample\_submission.csv] a sample submission file in the correct format.
\end{description}

\begin{table}[htbp]
	\label{tbl:Attribute Information}
	\centering
	\caption{Attribute Information}
	\begin{tabular}{llllll}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		Attributes & Information                                                                            \\
		\hline
		id   & a unique identifier for each tweet                                                               \\
		text    & the text of the tweet                                                                         \\
		location     & the location the tweet was sent from (may be blank)                                      \\
		keyword     & a particular keyword from the tweet (may be blank)                                        \\
		target    &  in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)    \\                                               \\
		\hline
		%\bottomrule
	\end{tabular}
\end{table}

\subsection{Problem Analysis}

\subsubsection{Problem Possible Solutions}
\

There are many machine learning models and algorithms 
that can solve NLP problems, such as Word2vec, 
FastText, BERT, etc. Word embedding and text cleaning 
through pre-trained models may greatly improve 
prediction accuracy. Then use BERT to 
get the final prediction result. 


\subsubsection{Evaluation Methods}


Before experiment, determine the evaluation methods
to assess the model performance is very important,
Submissions are evaluated using F1 
between the predicted and expected answers.

\begin{itemize}
	\item
	F1 is calculated as follows: \\[5 pt]
	$F _ { 1 } = 2 * \frac { \text { precision } * \text { recall } } { \text { precision } + \text {recall} }$
	\\[5 pt]
	\item
	where: \\[5 pt]
	$\begin{aligned} \text {precision} & = \frac { T P } { T P + F P } \\ \text {recall} & = \frac { T P } { T P + F N } \end{aligned}$
\end{itemize}


\section{Exploratory Data Analysis} \label{sec-data_exploration}

\subsection{Keyword and Location}
\

\subsubsection{Missing Values}
Both training and test set have same ratio 
of missing values in keyword and location.
the fllowings ~\cref{fig:missing-values-of-keyword-and-location} are the  
missing values of keyword and location.

\begin{itemize}
	\item
    0.8\% of keyword is missing in both training and test set
	\item
	33\% of location is missing in both training and test set
\end{itemize}


\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.3]{"Figures/Missing Values of Keyword and Location.pdf"}
	\caption{Missing Values of Keyword and Location}
	\label{fig:missing-values-of-keyword-and-location}
\end{figure}

Since missing value ratios between training and test set 
are too close, they are most probably taken from the 
same sample. Missing values in those features are 
filled with no\_keyword and no\_location respectively.

\subsubsection{Cardinality and Target Distribution}

Locations are not automatically generated, they are user inputs. 
That's why `location` is very dirty and there are too many 
unique values in it. It shouldn't be used as a feature.

\begin{itemize}
	\item
	Number of unique values in keyword: 222 (Training) - 222 (Test)
	\item
	Number of unique values in location: 3342 (Training) - 1603 (Test)
\end{itemize}

The fllowings ~\cref{fig:Target-Distribution-in-Keywords} shows that 
there is signal in `keyword` because some of those words can only 
be used in one context. Keywords have very different tweet counts 
and target means. `keyword` can be used as a feature by itself or 
as a word added to the text. Every single keyword in training set 
exists in test set. If training and test set are from the same 
sample, it is also possible to use target encoding on `keyword`.

\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.3]{"Figures/Target Distribution in Keywords.pdf"}
	\caption{Target Distribution in Keywords}
	\label{fig:Target-Distribution-in-Keywords}
\end{figure}

\subsection{Meta Features}
\

Distributions of meta features in classes and datasets 
can be helpful to identify disaster tweets. It looks 
like disaster tweets are written in a more formal way 
with longer words compared to non-disaster tweets 
because most of them are coming from news agencies. 
Non-disaster tweets have more typos than disaster tweets 
because they are coming from individual users. The meta 
features used for the analysis are;

\begin{description}
	\item[word\_count] number of words in text.
	\item[unique\_word\_count] number of unique words in text.
	\item[stop\_word\_count] number of stop words in text.
	\item[url\_count] number of urls in text.
	\item[mean\_word\_length] average character count in words.
	\item[char\_count] number of characters in text.
	\item[punctuation\_count] number of punctuations in text.
	\item[hashtag\_count] number of hashtags "\#" in text.
	\item[mention\_count] number of mentions "@" in text.
\end{description}

The fllowings ~\cref{fig:Meta-feature-target-distribution} shows that 
all of the meta features have very similar distributions in 
training and test set which also proves that training and 
test set are taken from the same sample.

All of the meta features have information about target as 
well, but some of them are not good enough such as `url\_count`, 
`hashtag\_count` and `mention\_count`.

On the other hand, `word\_count`, `unique\_word\_count`, 
`stop\_word\_count`, `mean\_word\_length`, `char\_count`, 
`punctuation\_count` have very different distributions 
for disaster and non-disaster tweets. Those features 
might be useful in models.

\begin{figure}[tbph]
	\centering
	\includegraphics[scale=0.1]{"Figures/Meta feature target distribution.pdf"}
	\caption{Meta feature target distribution}
	\label{fig:Meta-feature-target-distribution}
\end{figure}



\subsection{Target and N-grams}
\subsubsection{Target}
\

The fllowings ~\cref{fig:Target-distribution} shows that 
class distributions are 57\% for 0 (Not Disaster) and 43\% for 1 (Disaster). 
Classes are almost equally separated so they don't require any 
stratification by `target` in cross-validation. 


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Target distribution.pdf}
	\caption{Target distribution}
	\label{fig:Target-distribution}
\end{figure}

\subsubsection{N-grams}
\paragraph{Unigrams}
The fllowings ~\cref{fig:Unigrams} shows that:
\begin{itemize}
	\item
	Most common unigrams exist in both classes are mostly punctuations, 
	stop words or numbers. It is better to clean them before modelling 
	since they don't give much information about `target`.
	\item
	Most common unigrams in disaster tweets are already giving 
	information about disasters. It is very hard to use some 
	of those words in other contexts.
	\item
	Most common unigrams in non-disaster tweets are verbs. 
	This makes sense because most of those sentences have informal 
	active structure since they are coming from individual users.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Unigrams.pdf}
	\caption{Unigrams}
	\label{fig:Unigrams}
\end{figure}

\paragraph{Bigrams}
There are no common bigrams exist in both classes  because the context is clearer.
The fllowings ~\cref{fig:Bigrams} shows that:
\begin{itemize}
	\item
	Most common bigrams in disaster tweets are giving more information about 
	the disasters than unigrams, but punctuations have to be stripped from words.
	\item
	Most common bigrams in non-disaster tweets are mostly about reddit or 
	youtube, and they contain lots of punctuations. Those punctuations have to be cleaned out of words as well.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Bigrams.pdf}
	\caption{Bigrams}
	\label{fig:Bigrams}
\end{figure}

\paragraph{Trigrams}
There are no common trigrams exist in both classes  because the context is clearer.
The fllowings ~\cref{fig:Bigrams} shows that:
\begin{itemize}
	\item
	Most common trigrams in **disaster** tweets are very similar to bigrams. 
	They give lots of information about disasters, but they may not provide any 
	additional information along with bigrams.
	\item
	Most common trigrams in **non-disaster** tweets are also very similar to bigrams, and they contain even more punctuations.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{figures/Trigrams.pdf}
	\caption{Trigrams}
	\label{fig:Trigrams}
\end{figure}


\section{Embeddings and Text Cleaning}
\subsubsection{Embeddings Coverage}

When you have pre-trained embeddings, doing standard preprocessing 
steps  might not be a good idea because some of the valuable 
information can be lost. It is better to get vocabulary as close 
to embeddings as possible. In order to do that, train vocab and 
test vocab are created by counting the words in tweets.

Text cleaning is based on the embeddings below:

\begin{itemize}
	\item GloVe-300d-840B
	\item FastText-Crawl-300d-2M 
\end{itemize}

The fllowings ~\cref{tab:Embeddings-cover} shows that:
\begin{table}[htbp]
	\centering
	\caption{Embeddings cover}
	\begin{tabular}{lcccc}
		\toprule
		& \multicolumn{2}{c}{Training Set} & \multicolumn{2}{c}{Test Set} \\
		\midrule
		& vocabulary & text  & vocabulary & text \\
		\midrule
		Glove & 52.06\% & 82.68\% & 57.21\% & 81.85\% \\
		FastText & 51.52\% & 81.84\% & 56.55\% & 81.12\% \\
		\bottomrule
	\end{tabular}%
	\label{tab:Embeddings-cover}%
\end{table}%
                  
	
\subsubsection{XGBoost}
\
 
XGBoost is to establish K regression trees 
so that the predicted value of 
the tree group is as close as possible to 
the true value (accuracy) and 
has the greatest generalization ability. 
From a mathematical point of view, 
this is a functional optimization, multi-target.

\begin{description}
	\item[learning\_rate]  control the speed of each update
	\item[n\_estimators] number of iterations
	\item[max\_depth] the depth of tree
	\item[gamma] penalty factor%æƒ©ç½šç³»æ•°
	\item[subsample] the proportion of data used in 
		all training sets when training each tree
	\item[colsample\_bytree] the proportion of features used 
		in all trees when training each tree
	\end{description}

\subsubsection{LSTM}
\

Long short-term memory (LSTM) is an artificial 
recurrent neural network (RNN) 
architecture used in the field of 
deep learning. LSTM has feedback connections. 
It can not only process single data 
points (such as images), but also entire 
sequences of data (such as speech or video). 
	
	
\begin{description}
	\item[units] Output dimension 
		the number of neurons in the i-th hidden layer
	\item[activation] activation function
	\item[recurrent\_activation] Activation function applied to the loop step
	\item[use\_bias] Boolean, whether to use bias term
\end{description}

\subsubsection{Linear regression}
\

Linear regression is a linear approach to 
modeling the relationship between a scalar 
response (or dependent variable) and one 
or more explanatory variables (or 
independent variables). 


\begin{description}
	\item[fit\_intercept] whether to calculate the intercept for this model. If 
	set to False, no intercept will be used in calculations 
	the number of neurons in the i-th hidden layer
	\item[normalize] This parameter is ignored when fit\_intercept is set to 
	False
	\item[copy\_X] If True, X will be copied; else, it may be overwritten
	\item[n\_jobs] The number of jobs to use for the computation
\end{description}

\subsubsection{KNN}
\

k-NN is a type of instance-based learning, 
or lazy learning, where the function is 
only approximated locally and all 
computation is deferred until classification. 


\begin{description}
	\item[n\_neighbors] Number of neighbors to use by default for kneighbors 
	queries
	\item[weights] weight function used in prediction. Possible values
	\item[algorithm] Algorithm used to compute the nearest neighbors
	\item[leaf\_size] Leaf size passed to BallTree or KDTree
\end{description}

\subsection{Ensemble Model}
\

To combine the base models as 1st level 
model predictions, I'll use a simple 
linear regression. As I'm only feeding 
the model with predictions 
I don't need a complex model.

\section{Experiment and Analysis}

In the Data Exploration, 
I has created some new feaures 
and found some outliers.
Because the number of outliers is very small, 
after ignoring the outliers, the features 
are selected for experiments based on their importance.
And use the trained models as 
the base models of ensemble model,
then do experiment.
%\WBJianginMarker

\subsection{Base Models Training Result}

The following are the best parameters and 
the best Score in training of 
the base models. 

\begin{itemize}
	\item Best Parameters of Models
	\begin{description}
		\item[RandomForest] 'n\_jobs': '-1', 'max\_depth': 15, 
		'random\_state': 42, 'n\_estimators': 25
		\item[XGBoost] 'max\_depth':10, 
		'subsample':1,
		'min\_child\_weight':0.5,
		'eta':0.3, 
		'num\_round':1000, 
		'seed':1,
		'silent':0,
		'eval\_metric':'rmse'
		\item[LSTM] "batch\_size":128,
		"verbose":2,
		"epochs":10
		\item[Linear regression] No other parameters required
		\item[KNN] n\_neighbors=9, leaf\_size=13, n\_jobs=-1
	\end{description}


\subsection{Forecast Result of Base Models}
\

From the  ~\Cref{tbl:best_score_base_models_old},
it shows that the rmse of 
each model, RandomForest and XGBoost 
perform better, and there is not much 
difference between the other models.

\begin{table}[h]  \centering
	\caption{Best Score of the Base Models}
	\label{tbl:best_score_base_models_old}
	\begin{tabular}{ccccccc}
		%\bottomrule
		\toprule
		& RandomForest & XGBoost & LSTM & Linear regression & KNN\\
		\midrule
		Train rmse & 0.8358 & 0.8327 & 0.9276 & 0.8572 & 0.6976\\
		Validation rmse & 0.8810 & 0.8959 & 0.6611 & 0.8806 & 0.8946\\
		\bottomrule
	\end{tabular}
\end{table}
\end{itemize}

\subsection{Forecast Result of Ensemble Model }
\

Ensemble model means using 
more than 1 model to finish the prediction.
The train rmse is 0.764973649571408.

\section{Conclusion}

\begin{itemize}
	\item Exploratory data analysis is 
	very important for the competition,
	Discover the imperfections of the 
	data and have a certain understanding 
	of the overall appearance of the data, 
	which will help later modeling and analysis. 
	\item The data that we have,
	needed processed in many cases.
	Data preprocessing includes 
	deal with missing data and outliers, 
	We must think carefully about the outliers, 
	such as ignoring them.
	\item The most important thing is
	feature engineering.
	We have to think carefully and 
	deal with outliers, such as ignoring 
	or deleting them.
	\item There is no best model, 
	only the best model. We should 
	try as many models as possible to 
	get the best prediction results. 
	\item Feature engineering is very 
	important and even plays a decisive 
	role in this competition.
	\item The Ensemble model may perform better 
	than a single model when dealing 
	with some complex problems.	
\end{itemize}

%\lstset{language=python}         
%\begin{lstlisting}[frame=single]  % Start your code-block
%rf = RandomForestClassifier(random_state = 0)
%clf = GridSearchCV(rf, param_grid = params, scoring = accuracy_scorer, cv = 10, n_jobs = -1)
%clf.fit(X_train, y_train)
%y_pred = clf.predict(X_test)
%\end{lstlisting}










