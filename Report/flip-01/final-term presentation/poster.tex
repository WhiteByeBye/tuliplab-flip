%%
%% This is file `tikzposter-template.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% tikzposter.dtx  (with options: `tikzposter-template.tex')
%%
%% This is a generated file.
%%
%% Copyright (C) 2014 by Pascal Richter, Elena Botoeva, Richard Barnard, and Dirk Surmann
%%
%% This file may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either
%% version 2.0 of this license or (at your option) any later
%% version. The latest version of this license is in:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% and version 2.0 or later is part of all distributions of
%% LaTeX version 2013/12/01 or later.
%%


\documentclass{tikzposter} %Options for format can be included here

\usepackage{todonotes}

\usepackage[tikz]{bclogo}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[absolute]{textpos}
\usepackage[it]{subfigure}
\usepackage{graphicx}
\usepackage{cmbright}
%\usepackage[default]{cantarell}
%\usepackage{avant}
%\usepackage[math]{iwona}
\usepackage[math]{kurier}
\usepackage[T1]{fontenc}


%% add your packages here
\usepackage{hyperref}
% for random text
\usepackage{lipsum}
\usepackage[english]{babel}
\usepackage[pangram]{blindtext}

\colorlet{backgroundcolor}{blue!10}

 % Title, Author, Institute
\title{NLP with Disaster Tweets}
\author{Rongxin Xu}
\institute{Hunan University, China
}
%\titlegraphic{logos/tulip-logo.eps}

%Choose Layout
\usetheme{Wave}

%\definebackgroundstyle{samplebackgroundstyle}{
%\draw[inner sep=0pt, line width=0pt, color=red, fill=backgroundcolor!30!black]
%(bottomleft) rectangle (topright);
%}
%
%\colorlet{backgroundcolor}{blue!10}

\begin{document}


\colorlet{blocktitlebgcolor}{blue!23}

 % Title block with title, author, logo, etc.
\maketitle

\begin{columns}
 % FIRST column
\column{0.5}% Width set relative to text width

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
 %\block{Main Objectives}{
%  	      	\begin{enumerate}
%  	      	\item Formalise research problem by extending \emph{outlying aspects mining}
%  	      	\item Proposed \emph{GOAM} algorithm is to solve research problem
%  	      	\item Utilise pruning strategies to reduce time complexity
%  	      	\end{enumerate}
%%  	      \end{minipage}
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Introduction}{
    This is a problem with natural language processing.
    The ubiquitousness of smartphones enables 
    people to announce an emergency they’re 
    observing in real-time. So the target is
    Predict whether a real disaster has occurred 
    based on keywords, location, and twitter text.
  		
  	\begin{description}
  		\item[id] unique identifier for each tweet.
  		\item[text] the text of the tweet.
  		\item[location] the location the tweet was sent from (may be blank).
  		\item[keyword] a particular keyword from the tweet (may be blank).
  		\item[target] in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0).
  	\end{description}
    There are many machine learning models and algorithms 
    that can solve NLP problems, such as Word2vec, 
    FastText, BERT, etc. Word embedding and text cleaning 
    through pre-trained models may greatly improve 
    prediction accuracy. Then use BERT to 
    get the final prediction result. 
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Meta Features}{
	 The meta features used for the analysis are:
	\begin{description}
		\item[word\_count] number of words in text.
		\item[unique\_word\_count] number of unique words in text.
		\item[stop\_word\_count] number of stop words in text.
		\item[url\_count] number of urls in text.
		\item[mean\_word\_length] average character count in words.
		\item[char\_count] number of characters in text.
		\item[punctuation\_count] number of punctuations in text.
		\item[hashtag\_count] number of hashtags "\#" in text.
		\item[mention\_count] number of mentions "@" in text.
	\end{description}
	\begin{center}
		\includegraphics[width=.3\linewidth]{Figures/Meta feature target distribution1.pdf}
		\quad\includegraphics[width=.3\linewidth]{Figures/Meta feature target distribution2.pdf}
		\quad\includegraphics[width=.3\linewidth]{Figures/Meta feature target distribution3.pdf}
	\end{center}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

%\note{Note with default behavior}

%\note[targetoffsetx=12cm, targetoffsety=-1cm, angle=20, rotate=25]
%{Note \\ offset and rotated}

 % First column - second block


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Exploratory Data Analysis}{
  	Locations are not automatically generated, they are user inputs.	That's why `location` is very dirty and there are too many unique values in it. It shouldn't be used as a feature.
  	\begin{center}
  		\includegraphics[width=.4\linewidth]{Figures/Missing Values of Keyword and Location.pdf}
  	\end{center}
  there is signal in `keyword` because some of those words can only
  be used in one context. Keywords have very different tweet counts and target means. `keyword` can be used as a feature by itself or as a word added to the text.
  \begin{center}
  	\includegraphics[height=.4\linewidth]{Figures/Target Distribution in Keywords.pdf}
  \end{center}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% SECOND column
\column{0.5}
 %Second column with first block's top edge aligned with with previous column's top.

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Embeddings and Text Cleaning}{
When you have pre-trained embeddings, doing standard preprocessing 
steps  might not be a good idea because some of the valuable 
information can be lost. It is better to get vocabulary as close 
to embeddings as possible. In order to do that, train vocab and 
test vocab are created by counting the words in tweets.
Text cleaning is based on the embeddings below:
\begin{itemize}
	\item GloVe-300d-840B
	\item FastText-Crawl-300d-2M 
\end{itemize}
The most common type of words that require cleaning in `oov` 
have punctuations at the start or end. Those words doesn't have 
embeddings because of the trailing punctuations.
\begin{center}
	\begin{tabular}{cccc}
		\toprule
		\multicolumn{2}{c}{Special characters} & \multicolumn{2}{c}{Contractions} \\
		\midrule
		\textbackslash{}x89ÛÏWhen &  When & he's  &  he is \\
		China\textbackslash{}x89Ûªs &  China's & there's &  there is \\
		let\textbackslash{}x89Ûªs &  let's & We're &  We are \\
		fromåÊwounds &  from wounds & That's &  That is \\
		JapÌ\_n &  Japan & won't &  will not \\
		Ì©    &  e    & they're &  they are \\
		SuruÌ¤ &  Suruc & Can't &  Cannot \\
		å£3million &  3 million & wasn't &  was not \\
		\cmidrule{1-2}    \multicolumn{2}{c}{Character entity references} & don\textbackslash{}x89Ûªt &  do not \\
		\cmidrule{1-2}    \&gt  & , >   & aren't &  are not \\
		\&lt  & , <   & isn't &  is not \\
		\&amp & , \&  & What's &  What is \\
		……    & ……    & ……    & …… \\
		\bottomrule
	\end{tabular}%
\end{center}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
% Second column - first block


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block[titleleft]{Model and Evaluation}
{
This model uses the implementation of BERT from the TensorFlow Models 
repository on GitHub at `tensorflow/models/official/nlp/bert`. It uses 
L=12 hidden layers (Transformer blocks), a hidden size of H=768, and A=12 attention heads.
\begin{itemize}
	\item bert\_en\_uncased\_L-12\_H-768\_A-12/1
\end{itemize}
\begin{center}
	Evaluation\\
	\includegraphics[width=0.8\linewidth]{"Figures/Evaluation.pdf"}
\end{center}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% Second column - second block
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block[titlewidthscale=1, bodywidthscale=1]
{Conclusion}
{
\begin{itemize}
	\item Don't use standard preprocessing steps like stemming or 
	stopword removal when you have pre-trained embeddings.
	\item Get your vocabulary as close to the embeddings as possible.
	\item Text cleaning requires patience and meticulousness. 
	\item BERT does have an advantage over other models when 
	used properly on NLP problems.
\end{itemize}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% Bottomblock
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}
\note[targetoffsetx=8cm, targetoffsety=-4cm, angle=30, rotate=15,
radius=2cm, width=.26\textwidth]{
Acknowledgement
\begin{itemize}
    \item
    International Cooperation Project (Y7Z0511101)
    of IIE,
    Chinese Academy of Sciences
 \end{itemize}
}

%\note[targetoffsetx=8cm, targetoffsety=-10cm,rotate=0,angle=180,radius=8cm,width=.46\textwidth,innersep=.1cm]{
%Acknowledgement
%}

%\block[titlewidthscale=0.9, bodywidthscale=0.9]
%{Acknowledgement}{
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

\end{columns}


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
%[titleleft, titleoffsetx=2em, titleoffsety=1em, bodyoffsetx=2em,%
%roundedcorners=10, linewidth=0mm, titlewidthscale=0.7,%
%bodywidthscale=0.9, titlecenter]

%\colorlet{noteframecolor}{blue!20}
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}
\note[targetoffsetx=-13cm, targetoffsety=-12cm,rotate=0,angle=180,radius=8cm,width=.96\textwidth,innersep=.4cm]
{
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=24cm]{logos/tulip-wordmark.eps}
\end{minipage}
\begin{minipage}{0.7\linewidth}
{ \centering
 The $11^{th}$ International Conference on Knowledge Science,
  Engineering and Management (KSEM 2018),
  17-19/08/2018, Changchun, China
}
\end{minipage}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


\end{document}

%\endinput
%%
%% End of file `tikzposter-template.tex'.
